{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a753ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Import Required Packages\n",
    "# ========================\n",
    "import os, sys\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import casadi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "\n",
    "sys.path.append(os.path.abspath(\"mcmc\"))\n",
    "from mcmc_sampling import create_hmc_sampler\n",
    "\n",
    "# Local Debugging flag; remove when all tested\n",
    "_DEBUG = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a650f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_normal_log_probability(z, mean, cov):\n",
    "    \"\"\"\n",
    "    Evaluate and return the log-probability of a (unnormalized) multivariate normal distribution.\n",
    "\n",
    "    :param z: value of the multivariate normal random variable\n",
    "    :param mean: (1d array/list); mean of the normal distribution\n",
    "    :param cov: (2D array) covariance matrix\n",
    "\n",
    "    :returns: log-probability of a (unnormalized) multivariate normal distribution\n",
    "    \"\"\"\n",
    "    z    = np.asarray(z, dtype=float).flatten()\n",
    "    mean = np.asarray(z, dtype=float).flatten()\n",
    "    cov = np.asarray(cov, dtype=float).squeeze()\n",
    "\n",
    "    # Assert/Check input types/shapes\n",
    "    assert mean.size == z.size, \"mismatch between variable and mean sizes/shapes\"\n",
    "    if mean.size > 1:\n",
    "        assert cov.shape == (z.size,z.size), \"mismatch between variable and covariances `cov` shape\"\n",
    "\n",
    "    elif mean.size == 1:\n",
    "        assert cov.size == 1, \"mismatch between variable and covariances `cov` shape\"\n",
    "        cov = np.reshape(cov, (1, 1))\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid covariance matrix shape!\")\n",
    "        raise AssertionError\n",
    "\n",
    "    # Evaluate the log-probability\n",
    "    dev      = z - mean\n",
    "    scld_dev = np.linalg.inv(cov).dot(dev)\n",
    "    log_prob = - 0.5 * np.dot(dev, scld_dev)\n",
    "\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23697e0d-7eda-405b-9dcd-0b7f5449df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_num = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f5da72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_site_data(site_num, \n",
    "                   norm_fac=1.0,\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    Load site data\n",
    "\n",
    "    :returns:\n",
    "        -\n",
    "    \"\"\"\n",
    "    # Read data file\n",
    "    n=site_num\n",
    "    file=f\"data/calibration_{n}SitesModel.csv\"\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    \n",
    "    # Extract information\n",
    "    z_2017             = df[f'z_2017_{n}Sites'].to_numpy()\n",
    "    zbar_2017          = df[f'zbar_2017_{n}Sites'].to_numpy()\n",
    "    gamma              = df[f'gamma_{n}Sites'].to_numpy()\n",
    "    gammaSD            = df[f'gammaSD_{n}Sites'].to_numpy()\n",
    "    forestArea_2017_ha = df[f'forestArea_2017_ha_{n}Sites'].to_numpy()\n",
    "    theta              = df[f'theta_{n}Sites'].to_numpy()\n",
    "\n",
    "    # Normalize Z data\n",
    "    zbar_2017 /= norm_fac\n",
    "    z_2017    /= norm_fac\n",
    "    \n",
    "    return (zbar_2017,\n",
    "            gamma,\n",
    "            gammaSD,\n",
    "            z_2017,\n",
    "            forestArea_2017_ha,\n",
    "            theta\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06da816b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma [516.88557127 488.95668786 535.7933372  614.38476229 697.34085963\n",
      " 594.38440765 540.18969668 502.09011583 586.13789618 622.92870153\n",
      " 468.53543538 216.29945459 736.1126944  644.37719596 568.13204882\n",
      " 618.56342492 570.93769461 376.7589174  806.29456683 649.96115287\n",
      " 577.10631912 455.99483831 366.09394675 337.21852297 287.82630347]\n",
      "gammaSD [22.01380318 12.30650843 23.34479384 32.19887999 33.0552216  20.86507363\n",
      " 10.44879734  7.97210675 16.35650086 18.89356099  5.66673842  4.7426229\n",
      " 27.52295223 16.35653107 10.55158851 21.85866557 24.68789648  7.04434639\n",
      " 35.21752443 25.01272143 13.69821009  6.68635044  1.78628409 14.31382086\n",
      " 17.20154406]\n",
      "z_2017 [5.77092940e-07 5.58918518e-06 7.87674083e-04 9.64686174e-06\n",
      " 1.28634503e-04 1.62486537e-05 7.71270596e-05 4.13615376e-04\n",
      " 1.41563731e-03 2.97549040e-03 7.91141025e-03 3.11951500e-04\n",
      " 4.91859804e-04 5.48889538e-04 1.19367249e-03 1.41661798e-03\n",
      " 6.05514368e-03 6.74644202e-03 8.44104902e-05 2.44820053e-03\n",
      " 7.38590342e-03 8.45649924e-03 5.22554843e-03 8.60690505e-04\n",
      " 2.31193733e-03]\n",
      "gamma - gammaSD [494.87176808 476.65017943 512.44854336 582.1858823  664.28563803\n",
      " 573.51933402 529.74089934 494.11800908 569.78139532 604.03514054\n",
      " 462.86869696 211.55683169 708.58974217 628.02066489 557.58046031\n",
      " 596.70475936 546.24979813 369.71457101 771.0770424  624.94843144\n",
      " 563.40810902 449.30848787 364.30766265 322.90470211 270.62475941]\n",
      "zbar_2017 [0.00082784 0.00592396 0.0162693  0.00831711 0.01149272 0.00364199\n",
      " 0.02794187 0.02603153 0.02649412 0.02588343 0.01898131 0.00098645\n",
      " 0.02398373 0.02837916 0.02750545 0.0280807  0.0275304  0.00982846\n",
      " 0.00437791 0.00834653 0.0212187  0.02077097 0.01650749 0.00157716\n",
      " 0.00345833]\n"
     ]
    }
   ],
   "source": [
    "# Verbose Data check (eyeball check!)\n",
    "zbar_2017, gamma, gammaSD, \\\n",
    "            z_2017, forestArea_2017_ha, theta = load_site_data(site_num,norm_fac=1e+9)\n",
    "print(\"gamma\", gamma)\n",
    "print(\"gammaSD\", gammaSD)\n",
    "print(\"z_2017\", z_2017)\n",
    "print(\"gamma - gammaSD\", gamma - gammaSD)\n",
    "print(\"zbar_2017\", zbar_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "518ca24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_density_function(gamma_val,\n",
    "                         gamma_vals_mean,\n",
    "                         theta_vals,\n",
    "                   \n",
    "                         site_precisions,\n",
    "                         alpha,\n",
    "                         sol,\n",
    "                         X,\n",
    "                         Ua,\n",
    "                         Up,\n",
    "                         zbar_2017,\n",
    "                         forestArea_2017_ha,\n",
    "                         norm_fac,\n",
    "                         alpha_p_Adym,\n",
    "                         Bdym,\n",
    "                         leng,\n",
    "                         T,\n",
    "                         ds_vect,\n",
    "                         zeta,\n",
    "                         xi,\n",
    "                         kappa,\n",
    "                         pa,\n",
    "                         pf,\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Define a function to evaluate log-density of the objective/posterior distribution\n",
    "    Some of the input parameters are updated at each cycle of the outer loop (optimization loop),\n",
    "    and it becomes then easier/cheaper to udpate the function stamp and keep it separate here\n",
    "    \"\"\"\n",
    "    N          = X.shape[1] - 1\n",
    "    \n",
    "    gamma_val  = np.asarray(gamma_val).flatten()\n",
    "    gamma_size = gamma_val.size\n",
    "    x0_vals    = gamma_val.T.dot(forestArea_2017_ha) / norm_fac\n",
    "    X_zero     = np.sum(x0_vals) * np.ones(leng)\n",
    "    \n",
    "    \n",
    "    # shifted_X = zbar_2017 - sol.value(X)[0:gamma_size, :-1]\n",
    "    shifted_X  = sol.value(X)[0: gamma_size, :-1].copy()\n",
    "    for j in range(N): \n",
    "        shifted_X[:, j]  = zbar_2017 - shifted_X[:, j]\n",
    "    omega      = np.dot(gamma_val, alpha * shifted_X - sol.value(Up))\n",
    "    \n",
    "    X_dym      = np.zeros(T+1)\n",
    "    X_dym[0]   = np.sum(x0_vals)\n",
    "    X_dym[1: ] = alpha_p_Adym * X_zero  + np.dot(Bdym, omega.T)\n",
    "\n",
    "    z_shifted_X = sol.value(X)[0: gamma_size, :].copy()\n",
    "    scl = pa * theta_vals - pf * kappa\n",
    "    for j in range(N+1): z_shifted_X [:, j] *= scl\n",
    "    \n",
    "    term_1 = - casadi.sum2(np.reshape(ds_vect[0: T], (1, T)) * sol.value(Ua) * zeta / 2 )\n",
    "    term_2 = casadi.sum2(np.reshape(ds_vect[0: T], (1, T)) * pf * (X_dym[1: ] - X_dym[0: -1]))\n",
    "    term_3 = casadi.sum2(np.reshape(ds_vect, (1, N+1)) * casadi.sum1(z_shifted_X))\n",
    "    \n",
    "    obj_val = term_1 + term_2 + term_3\n",
    "    \n",
    "    gamma_val_dev   = gamma_val - gamma_vals_mean\n",
    "    norm_log_prob   =   - 0.5 * np.dot(gamma_val_dev,\n",
    "                                       site_precisions.dot(gamma_val_dev)\n",
    "                                       )\n",
    "    log_density_val = -1.0  / xi * obj_val + norm_log_prob\n",
    "\n",
    "    if _DEBUG:\n",
    "        print(\"Term 1: \", term_1)\n",
    "        print(\"Term 2: \", term_2)\n",
    "        print(\"Term 3: \", term_3)\n",
    "        print(\"obj_val: \", obj_val)\n",
    "        print(\"norm_log_prob\", norm_log_prob)\n",
    "        print(\"log_density_val\", log_density_val)\n",
    "\n",
    "    return log_density_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a21fbec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(\n",
    "    # Configurations/Settings\n",
    "    norm_fac          = 1e9,\n",
    "    delta_t           = 0.02,\n",
    "    alpha             = 0.045007414,\n",
    "    kappa             = 2.094215255,\n",
    "    pf                = 20.76,\n",
    "    pa                = 44.75,\n",
    "    xi                = 0.01,\n",
    "    zeta              = 1.66e-4*1e9,  # zeta := 1.66e-4*norm_fac  #\n",
    "    #\n",
    "    max_iter          = 200,\n",
    "    tol               = 0.01,\n",
    "    T                 = 200,\n",
    "    N                 = 200,\n",
    "    #\n",
    "    sample_size       = 1000,    # simulations before convergence (to evaluate the mean)\n",
    "    mode_as_solution  = False,   # If true, use the mode (point of high probability) as solution for gamma\n",
    "    final_sample_size = 100_00, # number of samples to collect after convergence\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Main function; putting things together\n",
    "\n",
    "    :param float tol: convergence tolerance\n",
    "    :param T:\n",
    "    :param N:\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Load sites' data\n",
    "    zbar_2017, gamma, gammaSD, \\\n",
    "        z_2017, forestArea_2017_ha, theta \\\n",
    "        = load_site_data(site_num,norm_fac=norm_fac)\n",
    "\n",
    "\n",
    "    # Evaluate Gamma values ()\n",
    "    gamma_1_vals  = gamma -  gammaSD\n",
    "    gamma_2_vals  = gamma +  gammaSD\n",
    "    gamma_size    = gamma.size\n",
    "\n",
    "    # Evaluate mean and covariances from site data\n",
    "    site_stdev       = gammaSD\n",
    "    site_covariances = np.diag(np.power(site_stdev, 2))\n",
    "    site_precisions  = np.linalg.inv(site_covariances)\n",
    "    site_mean        = gamma_1_vals/2 + gamma_2_vals/2\n",
    "\n",
    "    # Retrieve z data for selected site(s)\n",
    "    site_z_vals  = z_2017\n",
    "\n",
    "    # Initialize Gamma Values\n",
    "    gamma_vals      = gamma.copy()\n",
    "    gamma_vals_mean = gamma.copy()\n",
    "    gamma_vals_old  = gamma.copy()\n",
    "\n",
    "    # Theta Values\n",
    "    theta_vals  = theta\n",
    "\n",
    "    # Householder to track sampled gamma values\n",
    "    # gamma_vals_tracker       = np.empty((gamma_vals.size, sample_size+1))\n",
    "    # gamma_vals_tracker[:, 0] = gamma_vals.copy()\n",
    "    gamma_vals_tracker = [gamma_vals.copy()]\n",
    "\n",
    "    # Collected Ensembles over all iterations; dictionary indexed by iteration number\n",
    "    collected_ensembles = {}\n",
    "\n",
    "    # Track error over iterations\n",
    "    error_tracker = []\n",
    "\n",
    "    # Update this parameter (leng) once figured out where it is coming from\n",
    "    leng = 200\n",
    "    arr  = np.cumsum(\n",
    "             np.triu(\n",
    "             np.ones((leng, leng))\n",
    "         ),\n",
    "         axis=1,\n",
    "    ).T\n",
    "    Bdym         = (1-alpha) ** (arr-1)\n",
    "    Bdym[Bdym>1] = 0.0\n",
    "    Adym         = np.arange(1, leng+1)\n",
    "    alpha_p_Adym = np.power(1-alpha, Adym)\n",
    "\n",
    "    # Initialize Blocks of the A matrix those won't change\n",
    "    A  = np.zeros((gamma_size+2, gamma_size+2))\n",
    "    Ax = np.zeros(gamma_size+2)\n",
    "\n",
    "    # Construct Matrix B\n",
    "    B = np.eye(N=gamma_size+2, M=gamma_size, k=0)\n",
    "    B = casadi.sparsify(B)\n",
    "\n",
    "    # Construct Matrxi D constant blocks\n",
    "    D  = np.zeros((gamma_size+2, gamma_size))\n",
    "\n",
    "    # time step!\n",
    "    dt = T / N\n",
    "\n",
    "    # Other placeholders!\n",
    "    ds_vect = np.exp(- delta_t * np.arange(N+1) * dt)\n",
    "    ds_vect = np.reshape(ds_vect, (ds_vect.size, 1))\n",
    "\n",
    "    # Results dictionary\n",
    "    results = dict(\n",
    "        gamma_size=gamma_size,\n",
    "        tol=tol,\n",
    "        T=T,\n",
    "        N=N,\n",
    "        norm_fac=norm_fac,\n",
    "        delta_t=delta_t,\n",
    "        alpha=alpha,\n",
    "        kappa=kappa,\n",
    "        pf=pf,\n",
    "        pa=pa,\n",
    "        xi=xi,\n",
    "        zeta=zeta,\n",
    "        sample_size=sample_size,\n",
    "        final_sample_size=final_sample_size,\n",
    "        mode_as_solution=mode_as_solution,\n",
    "    )\n",
    "\n",
    "    # Initialize error & iteration counter\n",
    "    error = np.infty\n",
    "    cntr = 0\n",
    "\n",
    "    # Loop until convergence\n",
    "    while cntr < max_iter and error > tol:\n",
    "\n",
    "        # Update x0\n",
    "        x0_vals = gamma_vals * forestArea_2017_ha / norm_fac\n",
    "\n",
    "        # Construct Matrix A from new gamma_vals\n",
    "        A[: -2, :]        = 0.0\n",
    "        Ax[0: gamma_size] = - alpha * gamma_vals[0: gamma_size]\n",
    "        Ax[-1]            = alpha * np.sum(gamma_vals * zbar_2017)\n",
    "        Ax[-2]            = - alpha\n",
    "        A[-2, :]          = Ax\n",
    "        A[-1, :]          = 0.0\n",
    "        A = casadi.sparsify(A)\n",
    "\n",
    "        # Construct Matrix D from new gamma_vals\n",
    "        D[:, :]  = 0.0\n",
    "        D[-2, :] = -gamma_vals\n",
    "        D = casadi.sparsify(D)\n",
    "        \n",
    "        # Define the right hand side (symbolic here) as a function of gamma\n",
    "        gamma = casadi.MX.sym('gamma' , gamma_size+2)\n",
    "        up    = casadi.MX.sym('up', gamma_size)\n",
    "        um    = casadi.MX.sym('um', gamma_size)\n",
    "\n",
    "        rhs = (A @ gamma + B @ (up-um) + D @ up) * dt + gamma\n",
    "        f = casadi.Function('f', [gamma, um, up], [rhs])\n",
    "        \n",
    "\n",
    "        ## Define an optimizer and initialize it, and set constraints\n",
    "        opti = casadi.Opti()\n",
    "\n",
    "        # Decision variables for states\n",
    "        X = opti.variable(gamma_size+2, N+1)\n",
    "\n",
    "        # Aliases for states\n",
    "        Up = opti.variable(gamma_size, N)\n",
    "        Um = opti.variable(gamma_size, N)\n",
    "        Ua = opti.variable(1, N)\n",
    "\n",
    "        # 1.2: Parameter for initial state\n",
    "        ic = opti.parameter(gamma_size+2)\n",
    "\n",
    "        # Gap-closing shooting constraints\n",
    "        for k in range(N):\n",
    "            opti.subject_to(X[:, k+1] == f(X[:, k], Um[:, k], Up[:, k]))\n",
    "\n",
    "        # Initial and terminal constraints\n",
    "        opti.subject_to(X[:, 0] == ic)\n",
    "        opti.subject_to(opti.bounded(0,\n",
    "                                     X[0: gamma_size, :],\n",
    "                                     zbar_2017[0: gamma_size]\n",
    "                                     )\n",
    "                        )\n",
    "\n",
    "        # Objective: regularization of controls\n",
    "        for k in range(gamma_size):\n",
    "            opti.subject_to(opti.bounded(0, Um[k,:], casadi.inf))\n",
    "            opti.subject_to(opti.bounded(0, Up[k,:], casadi.inf))\n",
    "\n",
    "        opti.subject_to(Ua == casadi.sum1(Up+Um)**2)\n",
    "\n",
    "        # Set teh optimization problem\n",
    "        term1 = casadi.sum2(ds_vect[0: N, :].T * Ua * zeta / 2) \n",
    "        term2 = - casadi.sum2(ds_vect[0: N, :].T * (pf * (X[-2, 1: ] - X[-2, 0 :-1])))\n",
    "        term3 = - casadi.sum2(ds_vect.T * casadi.sum1( (pa * theta_vals - pf * kappa ) * X[0: gamma_size, :] ))\n",
    "        \n",
    "        opti.minimize(term1 + term2 + term3)\n",
    "\n",
    "        # Solve optimization problem\n",
    "        options               = dict()\n",
    "        options[\"print_time\"] = False\n",
    "        options[\"expand\"]     = True\n",
    "        options[\"ipopt\"]      = {'print_level':                      0,\n",
    "                                 'fast_step_computation':            'yes',\n",
    "                                 'mu_allow_fast_monotone_decrease':  'yes',\n",
    "                                 'warm_start_init_point':            'yes',\n",
    "                                 }\n",
    "        opti.solver('ipopt', options)\n",
    "        \n",
    "        opti.set_value(ic,\n",
    "                       casadi.vertcat(site_z_vals,\n",
    "                                      np.sum(x0_vals),\n",
    "                                      1),\n",
    "                       )\n",
    "        \n",
    "        if _DEBUG:\n",
    "            print(\"ic: \", ic)\n",
    "            print(\"site_z_vals: \", site_z_vals)\n",
    "            print(\"x0_vals: \", x0_vals)\n",
    "            print(\"casadi.vertcat(site_z_vals,np.sum(x0_vals),1): \", casadi.vertcat(site_z_vals,np.sum(x0_vals),1))\n",
    "        sol = opti.solve()\n",
    "\n",
    "        if _DEBUG:\n",
    "            print(\"sol.value(X)\", sol.value(X))\n",
    "            print(\"sol.value(Ua)\", sol.value(Ua))\n",
    "            print(\"sol.value(Up)\", sol.value(Up))\n",
    "            print(\"sol.value(Um)\", sol.value(Um))\n",
    "        \n",
    "        \n",
    "        ## Start Sampling\n",
    "        # Update signature of log density evaluator\n",
    "        log_density = lambda gamma_val: log_density_function(gamma_val=gamma_val,\n",
    "                                                             gamma_vals_mean=gamma_vals_mean,\n",
    "                                                             theta_vals=theta_vals,\n",
    "                                                             site_precisions=site_precisions,\n",
    "                                                             alpha=alpha,\n",
    "                                                             sol=sol,\n",
    "                                                             X=X,\n",
    "                                                             Ua=Ua,\n",
    "                                                             Up=Up,\n",
    "                                                             zbar_2017=zbar_2017,\n",
    "                                                             forestArea_2017_ha=forestArea_2017_ha,\n",
    "                                                             norm_fac=norm_fac,\n",
    "                                                             alpha_p_Adym=alpha_p_Adym,\n",
    "                                                             Bdym=Bdym,\n",
    "                                                             leng=leng,\n",
    "                                                             T=T,\n",
    "                                                             ds_vect=ds_vect,\n",
    "                                                             zeta=zeta,\n",
    "                                                             xi=xi,\n",
    "                                                             kappa=kappa,\n",
    "                                                             pa=pa,\n",
    "                                                             pf=pf,\n",
    "                                                             )\n",
    "\n",
    "        # Create MCMC sampler & sample, then calculate diagnostics\n",
    "        sampler = create_hmc_sampler(\n",
    "            size=gamma_size,\n",
    "            log_density=log_density,\n",
    "            #\n",
    "            burn_in=100,\n",
    "            mix_in=2,\n",
    "            symplectic_integrator='verlet',\n",
    "            symplectic_integrator_stepsize=1e-1,\n",
    "            symplectic_integrator_num_steps=3,\n",
    "            mass_matrix=1e+1,\n",
    "            constraint_test=lambda x: True if np.all(x>=0) else False,\n",
    "        )\n",
    "        gamma_post_samples = sampler.sample(\n",
    "            sample_size=sample_size,\n",
    "            initial_state=gamma_vals,\n",
    "            verbose=True,\n",
    "        )\n",
    "        gamma_post_samples = np.asarray(gamma_post_samples)\n",
    "\n",
    "        # Update ensemble/tracker\n",
    "        collected_ensembles.update({cntr: gamma_post_samples.copy()})\n",
    "\n",
    "        # Update gamma value\n",
    "        weight     = 0.25  # <-- Not sure how this linear combination weighting helps!\n",
    "        if mode_as_solution:\n",
    "            raise NotImplementedError(\"We will consider this in the future; trace sampled points and keep track of objective values to pick one with highest prob. \")\n",
    "            \n",
    "        else:\n",
    "            gamma_vals = weight * np.mean(gamma_post_samples, axis=0 ) + (1-weight) * gamma_vals_old\n",
    "        gamma_vals_tracker.append(gamma_vals.copy())\n",
    "\n",
    "        # Evaluate error for convergence check\n",
    "        error = np.max(np.abs(gamma_vals_old-gamma_vals) / gamma_vals_old)\n",
    "        error_tracker.append(error)\n",
    "        print(f\"Iteration [{cntr+1:4d}]: Error = {error}\")\n",
    "\n",
    "        # Exchange gamma values (for future weighting/update & error evaluation)\n",
    "        gamma_vals_old = gamma_vals\n",
    "\n",
    "        # Increase the counter\n",
    "        cntr += 1\n",
    "\n",
    "        results.update({'cntr': cntr,\n",
    "                        'error_tracker':np.asarray(error_tracker),\n",
    "                        'gamma_vals_tracker': np.asarray(gamma_vals_tracker),\n",
    "                        'collected_ensembles':collected_ensembles,\n",
    "                        })\n",
    "        pickle.dump(results, open('results.pcl', 'wb'))\n",
    "        \n",
    "        # Extensive plotting for monitoring; not needed really!\n",
    "        if False:\n",
    "            plt.plot(gamma_vals_tracker[-2], label=r'Old $\\gamma$')\n",
    "            plt.plot(gamma_vals_tracker[-1], label=r'New $\\gamma$')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            for j in range(gamma_size):\n",
    "                plt.hist(gamma_post_samples[:, j], bins=50)\n",
    "                plt.title(f\"Iteration {cntr}; Site {j+1}\")\n",
    "                plt.show()\n",
    "    \n",
    "    print(\"Terminated. Sampling the final distribution\")\n",
    "    # Sample (densly) the final distribution\n",
    "    final_sample = sampler.sample(\n",
    "        sample_size=final_sample_size,\n",
    "        initial_state=gamma_vals,\n",
    "        verbose=True,\n",
    "    )\n",
    "    final_sample = np.asarray(final_sample)\n",
    "    results.update({'final_sample': final_sample})\n",
    "    pickle.dump(results, open('results.pcl', 'wb'))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2566dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "\n",
      "=====================================================\n",
      "Started Sampling\n",
      "=====================================================\n",
      "\n",
      "HMC Iteration [ 536/2100]; Accept Prob: 1.00; --> Accepted? True  "
     ]
    }
   ],
   "source": [
    "results = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c79f95",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5016be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Error Results\n",
    "plt.plot(results['error_tracker'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Gamma Estimate Update\n",
    "for j in range(results['gamma_size']):\n",
    "    plt.plot(results['gamma_vals_tracker'][:, j], label=r\"$\\gamma_{%d}$\"%(j+1))\n",
    "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fbf2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Histograms\n",
    "for itr in results['collected_ensembles'].keys():\n",
    "    for j in range(results['gamma_size']):\n",
    "        plt.hist(results['collected_ensembles'][itr][:, j], bins=100)\n",
    "        plt.title(f\"Iteration {itr+1}; Site {j+1}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e7f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Histogram of the final sample\n",
    "for j in range(results['gamma_size']):\n",
    "    plt.hist(results['final_sample'][:, j], bins=100)\n",
    "    plt.title(f\"Final Sample; Site {j+1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3be0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7eb6cc-d49c-417a-bd2b-2d59b6438fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0944f72-8772-4c3c-b271-86f953ccdcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec13734-3a20-4e3f-af7a-5d1ab1f21427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
